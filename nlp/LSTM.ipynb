{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.1'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- os를 이용한 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9488\\3691449413.py:16: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup_text = BeautifulSoup(lower_text).get_text()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "train_data_path = './aclImdb/train/'\n",
    "test_data_path = './aclImdb/test/'\n",
    "\n",
    "positive_dir = 'pos/'\n",
    "negative_dir = 'neg/'\n",
    "\n",
    "def refine(text):\n",
    "    lower_text = text.lower()\n",
    "    soup_text = BeautifulSoup(lower_text).get_text()\n",
    "    refine_text =  re.sub('[^A-Za-z0-9가-힣 ]+', '', soup_text)\n",
    "    return refine_text\n",
    "\n",
    "def dir_to_dataset(filepath, inputs, labels, label):\n",
    "\n",
    "    for filename in os.listdir(filepath):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(filepath, filename),'r', encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "                str_data = refine(data)\n",
    "                inputs.append(str_data)\n",
    "                labels.append(label)\n",
    "\n",
    "train_text_data = []\n",
    "train_label_data = []\n",
    "\n",
    "dir_to_dataset(train_data_path + positive_dir, train_text_data, train_label_data, 1)\n",
    "dir_to_dataset(train_data_path + negative_dir, train_text_data, train_label_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_text_data), len(train_label_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9488\\3691449413.py:16: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup_text = BeautifulSoup(lower_text).get_text()\n"
     ]
    }
   ],
   "source": [
    "test_text_data = []\n",
    "test_label_data = []\n",
    "\n",
    "dir_to_dataset(test_data_path + positive_dir, test_text_data, test_label_data, 1)\n",
    "dir_to_dataset(test_data_path + negative_dir, test_text_data, test_label_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "print(len(test_text_data), len(test_label_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_data_train = list(zip(train_text_data, train_label_data))\n",
    "random.shuffle(comb_data_train)\n",
    "train_text_data, train_label_data = zip(*comb_data_train)\n",
    "\n",
    "comb_data_test = list(zip(test_text_data, test_label_data))\n",
    "random.shuffle(comb_data_test)\n",
    "test_text_data, test_label_data = zip(*comb_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 텍스트 데이터 전처리\n",
    " - (1) 텍스트 정제\n",
    " \n",
    "    - 소문자 변환\n",
    "\n",
    "    - html 태그 제거\n",
    "    \n",
    "    - 구두점 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (2) 토큰화 \n",
    " - vocab 파일을 word_index  변환 후 그것으로 토큰화 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "vocab = './aclImdb/imdb.vocab'\n",
    "\n",
    "# vocab 파일 열기\n",
    "with open(vocab, 'r') as f:\n",
    "    word = [line.strip() for line in f]\n",
    "# 단어 딕셔너리 생성\n",
    "word_index = {word : i for i, word in enumerate(word)}\n",
    "\n",
    "# 단어 딕셔너리 이용한 토큰화\n",
    "def index_tokenize(text):\n",
    "    words = text.split()\n",
    "    return [word_index.get(word, 0) for word in words]\n",
    "\n",
    "# 시퀀스 배열\n",
    "train_sequences = []\n",
    "# 트레인 셋 토큰화\n",
    "for data in train_text_data:\n",
    "    train_sequences.append(index_tokenize(data))\n",
    "\n",
    "test_sequences = []\n",
    "for data in test_text_data:\n",
    "    test_sequences.append(index_tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen of the damned is one of the best vampire movies i had ever seen the movie had suspense action and gore the combination of the fierce demanding attitude of the queen and the rock mood of our star very well acted by stuart townsend makes a wonderfully done movie that only this combination can create im always the one to give advice to my friends and family members on which movies are worthy of renting and when they ask me if queen of the damned is worthy i tell them its worthy of buying this movie is most for sure a musthave in all horror movie lovers homes\n",
      "[1545, 3, 0, 6204, 5, 26, 3, 0, 115, 1327, 99, 8, 65, 121, 107, 0, 15, 65, 812, 206, 1, 593, 0, 2134, 3, 0, 8454, 6048, 2110, 3, 0, 1545, 1, 0, 719, 1265, 3, 253, 324, 52, 71, 991, 30, 5166, 9536, 161, 2, 1616, 222, 15, 10, 60, 9, 2134, 66, 956, 4502, 204, 0, 26, 4, 192, 1973, 4, 54, 350, 1, 209, 1028, 18, 59, 99, 22, 1655, 3, 2689, 1, 50, 33, 896, 68, 42, 1545, 3, 0, 6204, 5, 1655, 8, 358, 92, 89, 1655, 3, 2604, 9, 15, 5, 87, 13, 241, 2, 0, 7, 28, 187, 15, 1806, 6182]\n",
      "1\n",
      "a beautiful filmjohn garfields character is a distant relative of les miserabless jean valjean while detective rains recalls victor hugos javertthe ruthless arm of lawlike in many films noirsthe city epitomizes evil whereas the country and the nature represents sanctuaryredemptionand a second chance for those whose life seems forever doomedbut even in the luminous daylightdanger may appear suddenlyas the excellent scene at the reservoir showsjohn garfield an actor whoas leonard maltin points outshould be rediscoveredive never been disappointed by any of his films except for his supporting part in gentlemans agreement  but it was not his faultgives a heartfelt sensitive performance and the audience sides with him as soon as he is unjustly accused the first sequence shows a rather unkind hypocrite personbut all his trials redeem him and how do we feel for him during the last scenes with detective rainscolorful characters grandma and the kids  add a lot of joie de vivre which is necessary humor is also present in the strip poker game as the dead end brats fleece a rich kidi recommend this movie\n",
      "[2, 297, 0, 0, 102, 5, 2, 3606, 3461, 3, 4965, 0, 1840, 21248, 134, 1209, 12689, 9497, 2115, 0, 0, 4190, 3174, 3, 0, 7, 106, 104, 0, 519, 21904, 435, 3060, 0, 661, 1, 0, 835, 3336, 0, 2, 330, 557, 13, 143, 602, 116, 180, 1378, 0, 53, 7, 0, 11861, 0, 191, 939, 0, 0, 309, 132, 29, 0, 17997, 0, 7493, 31, 264, 0, 3844, 11504, 730, 0, 25, 0, 110, 73, 657, 30, 97, 3, 23, 104, 531, 13, 23, 672, 170, 7, 0, 12223, 16, 6, 11, 19, 23, 0, 2, 5237, 2715, 229, 1, 0, 295, 2948, 14, 86, 12, 506, 12, 21, 5, 13563, 3544, 0, 84, 692, 276, 2, 239, 20276, 17869, 0, 28, 23, 5822, 5594, 86, 1, 85, 81, 69, 231, 13, 86, 302, 0, 226, 135, 14, 1209, 0, 101, 6531, 1, 0, 338, 735, 2, 169, 3, 21756, 842, 22473, 59, 5, 1622, 465, 5, 79, 979, 7, 0, 3481, 6172, 485, 12, 0, 353, 127, 20157, 37833, 2, 996, 0, 366, 9, 15]\n",
      "1\n",
      "when i saw the trailer for this film i said out loud to no one in particular this film is going to bomb i also said that about the matrix and look at what happened there now i am not a box office guru by any stretch but i usually have a pretty good gut about what is going to be good and what is going to really suck in this case i was blinded by my complete and utter apathy towards david duchovney let me put it to you a different way i dont like his as a person  from what i have read of him in interviews he is unbelievably premadonna like and he is full of himself considering all he has done is xfiles  or as an actor playing god was a really poor film but he came off thinking that for some reason he deserved big bucks on the big screen but i am happy to say that even though those things may still be true about the man return to me is delightful and has its heart in the right place bonnie hunt has directed a beautiful story and she has told it with class and grace this is one of the most romantic films i have seen and even though it may seem to be a bit sad and maudlin in its premise give it a chance and you will be hookedit has to be said  and this pains me to do so  that the reason this film works so well is because of the story and the cast duchovney and driver are so wonderful and believable here that i honestly wanted to cry along with them there is one particularly powerful scene when duchovney comes home after his wife has died and he slumps down on the floor of his house as it always does the family dog looks to the door to wait for his wife to come walking in she doesnt and with his shirt collar still stained with blood rob  duchovney  tells him that she is not coming home ever he then calls the dog over to him and they seem to share a cry together the dog lets out a small moan and then rob cries and this is one of the most realistic moments of pain i have ever seen in any character in any movie you can feel his pain and at that moment i forgot i was watching an actor that i generally dont like and i felt that i was watching someone that i knew moarn the loss of his beloved this is powerful stuffanother strength of the film is the supporting cast bonnie hunt has combined an ethnic melting pot of irish and italian characters that share a common bond they share a pub called oreilleys italian pub that is a delicious name all by itself and heading the diametric scale of clashing cultures is carol oconnor and robert loggia these are two proud old men that love their homeland but love their granddaughter and niece  i think it is  respectively and that is the character played by minnie driver this scenario is ripe for comedy and hunt doesnt miss anything herebonnie hunt and james belushi also share some funny moments together as the middle aged married couple and belushi gets top points as he accepts humility gracefully and shows off his ample keg of a stomach for laughs with his family consisting of three or four kids there is very little time for him and the wife to have quality time and again hunt handles this with perfect elegance this is a wonderful story of finding true love knowing how lucky you are to have true love and the power of friendship and family return to me is a wonderful romance and even though i still dont have a great admiration for david duchovney i have to admit that he was perfect in this role and i could not picture anyone else playing his character he was sensitive and believable and the movie was good because of him not just because of him but he sure added to the flavourif you are a sucker for a good romance and you want a good cry then this is the film for you 85 out of 10 i will see anything that bonnie hunt puts out with her in the directors chair\n",
      "[50, 8, 213, 0, 1438, 13, 9, 17, 8, 292, 44, 1354, 4, 55, 26, 7, 811, 9, 17, 5, 166, 4, 2268, 8, 79, 292, 10, 41, 0, 2585, 1, 163, 29, 45, 551, 37, 147, 8, 233, 19, 2, 994, 1038, 8484, 30, 97, 3129, 16, 8, 609, 24, 2, 178, 48, 7611, 41, 45, 5, 166, 4, 25, 48, 1, 45, 5, 166, 4, 62, 2740, 7, 9, 403, 8, 11, 9781, 30, 54, 573, 1, 2075, 16769, 912, 578, 36638, 271, 68, 265, 6, 4, 20, 2, 261, 93, 8, 5299, 36, 23, 12, 2, 389, 34, 45, 8, 24, 318, 3, 86, 7, 2927, 21, 5, 3720, 0, 36, 1, 21, 5, 374, 3, 300, 1026, 28, 21, 43, 222, 5, 0, 39, 12, 31, 264, 378, 530, 11, 2, 62, 328, 17, 16, 21, 365, 129, 518, 10, 13, 47, 270, 21, 1834, 195, 3493, 18, 0, 195, 277, 16, 8, 233, 653, 4, 131, 10, 53, 146, 143, 177, 191, 126, 25, 279, 41, 0, 124, 932, 4, 68, 5, 1842, 1, 43, 89, 512, 7, 0, 201, 262, 6836, 2292, 43, 521, 2, 297, 61, 1, 51, 43, 564, 6, 14, 794, 1, 1603, 9, 5, 26, 3, 0, 87, 714, 104, 8, 24, 107, 1, 53, 146, 6, 191, 296, 4, 25, 2, 219, 601, 1, 13493, 7, 89, 832, 192, 6, 2, 557, 1, 20, 76, 25, 0, 43, 4, 25, 292, 1, 9, 6971, 68, 4, 81, 35, 10, 0, 270, 9, 17, 475, 35, 71, 5, 82, 3, 0, 61, 1, 0, 172, 36638, 1, 2488, 22, 35, 369, 1, 834, 122, 10, 8, 1202, 455, 4, 1360, 352, 14, 92, 37, 5, 26, 553, 950, 132, 50, 36638, 256, 337, 98, 23, 307, 43, 1089, 1, 21, 30444, 183, 18, 0, 1832, 3, 23, 305, 12, 6, 204, 120, 0, 209, 880, 260, 4, 0, 1306, 4, 825, 13, 23, 307, 4, 210, 1253, 7, 51, 13487, 1, 14, 23, 4428, 9145, 126, 21053, 14, 524, 2001, 36638, 684, 86, 10, 51, 5, 19, 579, 337, 121, 21, 91, 1949, 0, 880, 128, 4, 86, 1, 33, 296, 4, 1443, 2, 1360, 287, 0, 880, 1583, 44, 2, 386, 16638, 1, 91, 2001, 6351, 1, 9, 5, 26, 3, 0, 87, 805, 367, 3, 1412, 8, 24, 121, 107, 7, 97, 102, 7, 97, 15, 20, 66, 231, 23, 1412, 1, 29, 10, 539, 8, 2678, 8, 11, 144, 31, 264, 10, 8, 1178, 5299, 36, 1, 8, 402, 10, 8, 11, 144, 268, 10, 8, 671, 0, 0, 1887, 3, 23, 2684, 9, 5, 950, 0, 2044, 3, 0, 17, 5, 0, 672, 172, 6836, 2292, 43, 2432, 31, 5656, 4213, 4678, 3, 2491, 1, 1119, 101, 10, 1443, 2, 1108, 1575, 33, 1443, 2, 8732, 477, 0, 1119, 8732, 10, 5, 2, 6272, 393, 28, 30, 394, 1, 5444, 0, 0, 2559, 3, 20096, 6544, 5, 3225, 0, 1, 633, 11397, 130, 22, 105, 2577, 164, 331, 10, 114, 63, 12597, 16, 114, 63, 10266, 1, 5823, 8, 100, 6, 5, 5053, 1, 10, 5, 0, 102, 246, 30, 36364, 2488, 9, 2605, 5, 11535, 13, 207, 1, 2292, 13487, 698, 224, 0, 2292, 1, 554, 4683, 79, 1443, 47, 158, 367, 287, 12, 0, 746, 3197, 983, 359, 1, 4683, 205, 405, 730, 12, 21, 5323, 13824, 12518, 1, 276, 129, 23, 7244, 39842, 3, 2, 2890, 13, 887, 14, 23, 209, 9840, 3, 288, 39, 690, 338, 37, 5, 52, 112, 57, 13, 86, 1, 0, 307, 4, 24, 481, 57, 1, 168, 2292, 5781, 9, 14, 392, 9947, 9, 5, 2, 369, 61, 3, 1484, 279, 114, 1228, 85, 2082, 20, 22, 4, 24, 279, 114, 1, 0, 652, 3, 1797, 1, 209, 932, 4, 68, 5, 2, 369, 853, 1, 53, 146, 8, 126, 5299, 24, 2, 83, 7979, 13, 578, 36638, 8, 24, 4, 937, 10, 21, 11, 392, 7, 9, 212, 1, 8, 95, 19, 418, 243, 313, 378, 23, 102, 21, 11, 2715, 1, 834, 1, 0, 15, 11, 48, 82, 3, 86, 19, 40, 82, 3, 86, 16, 21, 241, 1235, 4, 0, 0, 20, 22, 2, 6560, 13, 2, 48, 853, 1, 20, 176, 2, 48, 1360, 91, 9, 5, 0, 17, 13, 20, 0, 44, 3, 0, 8, 76, 64, 224, 10, 6836, 2292, 1399, 44, 14, 38, 7, 0, 871, 2986]\n",
      "1\n",
      "i just cant imagine any possible reasons why madsen and hopper wanted to be in this movie after reading the script they got blackmailed maybe or are they that badly out of money the main problem with the movie is that its boring the conversations between madsens and hoppers character are pointless just like the bored chatting between two buddies while drinking beer on a saturday night you never feel for any of the characters although madsens psycho killer is very likeable comparing to the other characters hopper always was a good actor and madsen does a fine job as the serial killer otherwise the acting is almost laughable there are about three scenes in the whole movie where something is actually happening each of them last about three minutes although the talking and thinking about murder and the nature of murderers scenes would have been interesting if they were scenes in a book the whole concept wouldve been interesting for a novel but a movie just cant bare with a story with that much inner thinking and so little action\n",
      "[8, 40, 2409, 807, 97, 589, 969, 133, 6633, 1, 4346, 455, 4, 25, 7, 9, 15, 98, 867, 0, 221, 33, 182, 17151, 266, 39, 22, 33, 10, 897, 44, 3, 267, 0, 283, 428, 14, 0, 15, 5, 10, 89, 344, 0, 3858, 194, 0, 1, 60206, 102, 22, 1103, 40, 36, 0, 1065, 15077, 194, 105, 3904, 134, 2766, 3619, 18, 2, 2260, 304, 20, 110, 231, 13, 97, 3, 0, 101, 251, 0, 2286, 434, 5, 52, 7017, 4326, 4, 0, 78, 101, 4346, 204, 11, 2, 48, 264, 1, 6633, 120, 2, 461, 282, 12, 0, 1393, 434, 873, 0, 109, 5, 215, 1270, 37, 22, 41, 288, 135, 7, 0, 217, 15, 113, 138, 5, 159, 1394, 245, 3, 92, 226, 41, 288, 225, 251, 0, 648, 1, 518, 41, 567, 1, 0, 835, 3, 7361, 135, 58, 24, 73, 214, 42, 33, 67, 135, 7, 2, 263, 0, 217, 1080, 53892, 73, 214, 13, 2, 630, 16, 2, 15, 40, 2409, 4188, 14, 2, 61, 14, 10, 72, 2503, 518, 1, 35, 112, 206]\n",
      "0\n",
      "late film critic gene siskel said that this movie shows how easy it is to make a movie he was giving it a compliment even though now that might have been taken as an insult these days even though i didnt always agree with gene siskel i agree with him here love jones is a shining example of how a love story should be realistic with real characters in real situationsthe story chronicles the ups and downs of the relationship between darius lovehalllarenz tate and ninania long larenz tate and nia long are more than just a beautiful couple on screen these two actually have chemistry together you can feel the vibe between these two whenever the are on screen and its fantasticbill bellamy is pretty funny as the deceitful wood and lisa nicole carson is great as ninas friend simone isaiah washington is just as great as dariuss close friend savon and i sigh every time i see him in a movie the guy is a great actor whose career is ruined by industry lowlifes and the childish games they play you can believe that he called the little weasel on greys anatomy out of his name but anybody that knows how follywood works knows better than to believe any official story from the place of make believe at any ratelove jones is a wonderful love story full of interesting and likable characters that are in realistic situations that anybody that has been in love can relate too you love these people because they are believable and are not portrayed as gangsters and tramps not one obscene stereotype can be found here contrast that with the romance movies of toady exactly in closing if you love black cinema then you would do well to own a copy of this movie\n",
      "[522, 17, 3502, 1849, 14678, 292, 10, 9, 15, 276, 85, 756, 6, 5, 4, 94, 2, 15, 21, 11, 716, 6, 2, 6978, 53, 146, 147, 10, 228, 24, 73, 606, 12, 31, 2299, 130, 483, 53, 146, 8, 9086, 204, 1004, 14, 1849, 14678, 8, 1004, 14, 86, 122, 114, 1434, 5, 2, 3417, 446, 3, 85, 2, 114, 61, 139, 25, 805, 14, 145, 101, 7, 145, 0, 61, 6113, 0, 4180, 1, 7140, 3, 0, 620, 194, 8830, 0, 15628, 1, 0, 203, 44380, 15628, 1, 0, 203, 22, 49, 70, 40, 2, 297, 359, 18, 277, 130, 105, 159, 24, 1138, 287, 20, 66, 231, 0, 8190, 194, 130, 105, 1876, 0, 22, 18, 277, 1, 89, 0, 33087, 5, 178, 158, 12, 0, 23079, 1939, 1, 2598, 4924, 9215, 5, 83, 12, 0, 433, 11234, 27527, 1853, 5, 40, 12, 83, 12, 0, 532, 433, 0, 1, 8, 8439, 171, 57, 8, 64, 86, 7, 2, 15, 0, 223, 5, 2, 83, 264, 602, 603, 5, 2182, 30, 1542, 31113, 1, 0, 3697, 1572, 33, 293, 20, 66, 255, 10, 21, 477, 0, 112, 28115, 18, 26469, 10997, 44, 3, 23, 393, 16, 1668, 10, 670, 85, 42218, 475, 670, 123, 70, 4, 255, 97, 4011, 61, 34, 0, 262, 3, 94, 255, 29, 97, 0, 1434, 5, 2, 369, 114, 61, 374, 3, 214, 1, 1405, 101, 10, 22, 7, 805, 1137, 10, 1668, 10, 43, 73, 7, 114, 66, 2119, 96, 20, 114, 130, 75, 82, 33, 22, 834, 1, 22, 19, 962, 12, 4412, 1, 25131, 19, 26, 9375, 4303, 66, 25, 248, 122, 2220, 10, 14, 0, 853, 99, 3, 46578, 594, 7, 2680, 42, 20, 114, 320, 420, 91, 20, 58, 81, 71, 4, 196, 2, 1003, 3, 9, 15]\n",
      "1\n",
      "i know its a movie but when it comes to portray real life in any matter it should be as faithful as possible im sorry but el misterio galndez isnt as accurate as it seems nor is the dominican republic depicted as it really is in fact it shocked me to see that the filming location for santo domingo was actually cuba and incredibly enough movies with cuban themes havana the lost city bitter sugar the godfather part ii were actually filmed in santo domingo so what happened here why did they shoot the movie in cuba instead of the dr the spanish dialogs with the cuban accent are horrible those are not dominicans on the historic level galndez would have never been hanged he might as well been shot decapitated or died from the inhumane torture hed been receiving then thrown his body in the caribbean sea but trujillo would have never ordered death by strangulation his sick mind wouldnt have allowed itacting isnt delivered as expected harvey keitel looks like hes just expecting a paycheck i prefer the leading actress in deep blue sea the rest of the cast would have been excellent in some cuban movie and the same goes for the selected shooting locationi suggest la fiesta del chivo the feast of the goat from bestselling author mario vargas llosa directed by his cousin luis llosa its a bit more realistic with dominican history the trujillo character is very well portrayed and the galindez incident is treated very briefly in this movie\n",
      "[8, 119, 89, 2, 15, 16, 50, 6, 256, 4, 1924, 145, 116, 7, 97, 543, 6, 139, 25, 12, 2703, 12, 589, 4502, 768, 16, 5439, 0, 0, 16533, 12, 1790, 12, 6, 180, 859, 5, 0, 17374, 6732, 2319, 12, 6, 62, 5, 7, 186, 6, 2366, 68, 4, 64, 10, 0, 1374, 1576, 13, 24838, 37773, 11, 159, 3913, 1, 928, 189, 99, 14, 6741, 1272, 12391, 0, 401, 519, 3019, 5862, 0, 3319, 170, 1464, 67, 159, 796, 7, 24838, 37773, 35, 45, 551, 122, 133, 117, 33, 1342, 0, 15, 7, 3913, 294, 3, 0, 844, 0, 1874, 3176, 14, 0, 6741, 1145, 22, 503, 143, 22, 19, 86894, 18, 0, 5553, 658, 0, 58, 24, 110, 73, 13372, 21, 228, 12, 71, 73, 315, 9670, 39, 1089, 34, 0, 20736, 1741, 0, 73, 5467, 91, 1336, 23, 636, 7, 0, 11199, 2028, 16, 0, 58, 24, 110, 5182, 329, 30, 23891, 23, 1170, 334, 19620, 24, 1609, 0, 16533, 2067, 12, 841, 3945, 8603, 260, 36, 9569, 40, 976, 2, 7814, 8, 2691, 0, 942, 498, 7, 935, 1379, 2028, 0, 348, 3, 0, 172, 58, 24, 73, 309, 7, 47, 6741, 15, 1, 0, 167, 258, 13, 0, 6541, 1172, 0, 1409, 1077, 0, 4693, 88812, 0, 6361, 3, 0, 7655, 34, 35687, 1878, 4209, 7449, 36783, 521, 30, 23, 3132, 5910, 36783, 89, 2, 219, 49, 805, 14, 17374, 458, 0, 0, 102, 5, 52, 71, 962, 1, 0, 0, 3765, 5, 1847, 52, 3271, 7, 9, 15]\n",
      "0\n",
      "one of the most poetic narrative films ever made wagonmaster is nonetheless a difficult film to immediately like i love this movie but i recommend seeing some of john fords other westerns before taking a look at this one the first time i saw it i was 18 years old and i hadnt seen too many other westerns and i hated it i thought it was incredibly boring i kept waiting for something to happen it took several years for me to love this picture first i fell in love with westerns in general  the traditions characters landscapes ways of talking etc  and that made me realize when i saw wagonmaster again that a lot is happening in it after alli also was simply a more experienced moviegoer at that point and had learned to appreciate visual storytelling and to listen to what each image was telling me wagonmaster is a very visual movie by one of the most visual of directors working near the peak of his careerthe movie is a celebration of a way of life and its subject matter is more emotional and interior than other ford westerns actually thats not really as accurate as saying that rather it has a lot less exterior action than the other westerns the other westerns have exterior action and interior emotion it quite beautifully places its mormon pioneers in the context of nature there are many shots of animals and children  not for any surface narrative purpose but for illustrating this idea that is why the movie can be called a poem it isnt about the surface story which barely exists nearly as much as it is about an emotional idea and it gets this idea across through composition editing sound and music in fact one could argue that this is a purer form of filmmaking because the images directly express the emotional idea of the film rather than having to first service a storygive this movie a chance and allow it to exist on its own terms not the terms of other westerns or other movies\n",
      "[26, 3, 0, 87, 4377, 1276, 104, 121, 90, 0, 5, 2833, 2, 851, 17, 4, 1192, 36, 8, 114, 9, 15, 16, 8, 366, 306, 47, 3, 290, 54604, 78, 2850, 155, 646, 2, 163, 29, 9, 26, 0, 84, 57, 8, 213, 6, 8, 11, 0, 150, 164, 1, 8, 85117, 107, 96, 106, 78, 2850, 1, 8, 1745, 6, 8, 198, 6, 11, 928, 344, 8, 798, 1027, 13, 138, 4, 569, 6, 544, 436, 150, 13, 68, 4, 114, 9, 418, 84, 8, 1521, 7, 114, 14, 2850, 7, 778, 0, 7555, 101, 4770, 737, 3, 648, 501, 1, 10, 90, 68, 888, 50, 8, 213, 0, 168, 10, 2, 169, 5, 1394, 7, 6, 98, 63787, 79, 11, 316, 2, 49, 2486, 17502, 29, 10, 211, 1, 65, 1988, 4, 1100, 1071, 2724, 1, 4, 1578, 4, 45, 245, 1432, 11, 1006, 68, 0, 5, 2, 52, 1071, 15, 30, 26, 3, 0, 87, 1071, 3, 871, 792, 779, 0, 4621, 3, 23, 0, 15, 5, 2, 5191, 3, 2, 93, 3, 116, 1, 89, 840, 543, 5, 49, 889, 1, 7550, 70, 78, 1703, 2850, 159, 1526, 19, 62, 12, 1790, 12, 639, 10, 239, 6, 43, 2, 169, 322, 6210, 206, 70, 0, 78, 2850, 0, 78, 2850, 24, 6210, 206, 1, 7550, 1367, 6, 175, 1251, 1307, 89, 6890, 13221, 7, 0, 1965, 3, 835, 37, 22, 106, 643, 3, 1322, 1, 408, 19, 13, 97, 2515, 1276, 1242, 16, 13, 17716, 9, 312, 10, 5, 133, 0, 15, 66, 25, 477, 2, 4782, 6, 16533, 41, 0, 2515, 61, 59, 1166, 2879, 729, 12, 72, 12, 6, 5, 41, 31, 889, 312, 1, 6, 205, 9, 312, 616, 140, 7095, 771, 469, 1, 220, 7, 186, 26, 95, 3843, 10, 9, 5, 2, 41000, 789, 3, 6394, 82, 0, 1174, 2473, 2539, 0, 889, 312, 3, 0, 17, 239, 70, 249, 4, 84, 2417, 2, 0, 9, 15, 2, 557, 1, 1680, 6, 4, 1751, 18, 89, 196, 1249, 19, 0, 1249, 3, 78, 2850, 39, 78, 99]\n",
      "1\n",
      "i dont see why everyone loves this film so much true it does have good intentions and meaning but you cannot compensate for such a poor script woody allen is a brilliant filmaker but im afraid this is just a piece of garbage its extremely predictable and the subject matter is all too visible i happen to be a huge woody allan fan and love most of his work but this i cannot recomend\n",
      "[8, 5299, 64, 133, 286, 1333, 9, 17, 35, 72, 279, 6, 120, 24, 48, 2905, 1, 1221, 16, 20, 545, 7819, 13, 136, 2, 328, 221, 2097, 1343, 5, 2, 509, 49193, 16, 4502, 1531, 9, 5, 40, 2, 404, 3, 1197, 89, 555, 702, 1, 0, 840, 543, 5, 28, 96, 4633, 8, 569, 4, 25, 2, 641, 2097, 8169, 327, 1, 114, 87, 3, 23, 160, 16, 9, 8, 545, 18324]\n",
      "0\n",
      "the prey has an interesting history unless you remember the ads for it in newspapers in june of 1984 you might have caught it on the movie channel back in summer 85 but little else is remembered the plot is your basic killer in the woods again but ironically this was filmed before friday the 13th the prey was actually shot sometime in 1978 according to one of the actors in an interview years later but released for about a week at some drive ins yes jim namans drive in showed this in june of 84 but it has a dated look to it maybe they released it so later on to cash in on all the other terror films the market was flooded with by 1984 now on the story it has some kind of back story a forest fire back in the 1940s leaves a lot of gypsies burned to death but one of their children survive our monster so flash forward to present day which would be 1978 we have an older middle age couple camping only to be dispatched by the monster the tag line for this picture claims its not human and its got an axe but an axe was only used in these first two killings now we have a bunch of teenagers who look like they in their mid 20s camping we all know they are the prey and the monster knocks them of one by one for an 80 minute movie it seems longer we also have a lot of wildlife footage to fill in voids for the 80 mins overall for being out into an 80s horror movie it looks way more 70s than ever hey the prey had potential to be a good horror killer in the woods movie but falls a little short it does however feature a pretty scary cool looking monster at the end and we have to wait till the last 2 minutes to see him side note the monster has gone on to star in the addams family movies in the 1990s\n",
      "[0, 5186, 43, 31, 214, 458, 868, 20, 360, 0, 8210, 13, 6, 7, 11045, 7, 4649, 3, 0, 20, 228, 24, 1019, 6, 18, 0, 15, 1252, 141, 7, 1442, 0, 16, 112, 313, 5, 2018, 0, 111, 5, 125, 1074, 434, 7, 0, 1346, 168, 16, 3566, 9, 11, 796, 155, 2437, 0, 0, 0, 5186, 11, 159, 315, 5915, 7, 0, 1738, 4, 26, 3, 0, 151, 7, 31, 2649, 150, 291, 16, 615, 13, 41, 2, 1246, 29, 47, 1471, 11923, 398, 1141, 0, 1471, 7, 1131, 9, 7, 4649, 3, 0, 16, 6, 43, 2, 1916, 163, 4, 6, 266, 33, 615, 6, 35, 291, 18, 4, 2231, 7, 18, 28, 0, 78, 2470, 104, 0, 2318, 11, 20734, 14, 30, 0, 147, 18, 0, 61, 6, 43, 47, 235, 3, 141, 61, 2, 2502, 959, 141, 7, 0, 0, 860, 2, 169, 3, 16837, 3789, 4, 329, 16, 26, 3, 63, 408, 1941, 253, 915, 35, 3428, 958, 4, 979, 250, 59, 58, 25, 0, 69, 24, 31, 886, 746, 571, 359, 7734, 60, 4, 25, 15528, 30, 0, 915, 0, 4787, 357, 13, 9, 418, 2370, 89, 19, 396, 1, 89, 182, 31, 6647, 16, 31, 6647, 11, 60, 335, 7, 130, 84, 105, 3373, 147, 69, 24, 2, 725, 3, 2276, 32, 163, 36, 33, 7, 63, 3677, 0, 7734, 69, 28, 119, 33, 22, 0, 5186, 1, 0, 915, 6542, 92, 3, 26, 30, 26, 13, 31, 0, 885, 15, 6, 180, 1163, 69, 79, 24, 2, 169, 3, 10838, 894, 4, 2196, 7, 78621, 13, 0, 0, 7305, 432, 13, 108, 44, 80, 31, 0, 187, 15, 6, 260, 93, 49, 0, 70, 121, 1341, 0, 5186, 65, 948, 4, 25, 2, 48, 187, 434, 7, 0, 1346, 15, 16, 703, 2, 112, 345, 6, 120, 184, 802, 2, 178, 614, 631, 284, 915, 29, 0, 127, 1, 69, 24, 4, 825, 2277, 0, 226, 0, 225, 4, 64, 86, 502, 863, 0, 915, 43, 797, 18, 4, 324, 7, 0, 36595, 209, 99, 7, 0, 0]\n",
      "0\n",
      "clifton webb is one of my favorites however mister scoutmaster is not one of his best his patented curmudgeon role seems forced and even unpleasant rather than funny the film itself is overflowing with mawkish sentimentality in addition the viewer is presented with numerous hamhanded references to religious faith and us patriotism that come off as overreverent rather than genuine clifton webb does his best with a poor script edmund gwenn plays yet another jovial clergyman and is given nothing to do the child actor lead is played by a talentless child who displays a flat affect throughout the entire film his sole claim to fame as a performer evidently is a bullfroglike low voice unusual for someone of his age however once youve heard it youve heard it and you dont need to hear it again unfortunately he is in the majority of the films scenes i find this child so irritating that i fast forward whenever he shows up since he has a lot of scenes in this film this means that i fast forward through a lot of the film there were and are so many talented child actors its a pity this film doesnt have any of them in it still clifton webb in the traditional broadbrimmed hat and shorts is a sight worth seeing\n",
      "[20257, 15259, 5, 26, 3, 54, 2574, 184, 7832, 0, 5, 19, 26, 3, 23, 115, 23, 15965, 26412, 212, 180, 883, 1, 53, 3874, 239, 70, 158, 0, 17, 394, 5, 24759, 14, 14429, 6432, 7, 1515, 0, 480, 5, 1302, 14, 1869, 0, 2012, 4, 1750, 1759, 1, 173, 10701, 10, 210, 129, 12, 0, 239, 70, 1986, 20257, 15259, 120, 23, 115, 14, 2, 328, 221, 7764, 45563, 289, 238, 153, 20277, 24855, 1, 5, 336, 156, 4, 81, 0, 482, 264, 468, 5, 246, 30, 2, 9746, 482, 32, 3757, 2, 1020, 4429, 453, 0, 424, 17, 23, 3618, 2240, 4, 2223, 12, 2, 4099, 5456, 5, 2, 0, 474, 575, 1672, 13, 268, 3, 23, 571, 184, 275, 0, 541, 6, 0, 541, 6, 1, 20, 5299, 347, 4, 808, 6, 168, 454, 21, 5, 7, 0, 2088, 3, 0, 104, 135, 8, 162, 9, 482, 35, 2147, 10, 8, 810, 958, 1876, 21, 276, 56, 227, 21, 43, 2, 169, 3, 135, 7, 9, 17, 9, 780, 10, 8, 810, 958, 140, 2, 169, 3, 0, 17, 37, 67, 1, 22, 35, 106, 1005, 482, 151, 89, 2, 2243, 9, 17, 13487, 24, 97, 3, 92, 7, 6, 126, 20257, 15259, 7, 0, 1984, 0, 2399, 1, 3072, 5, 2, 1663, 281, 306]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(test_text_data[i])\n",
    "    print(test_sequences[i])\n",
    "    print(test_label_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sequence 분석\n",
    "\n",
    " - 시퀀스 길이를 시각화하여 분석한 결과 시퀀스 길이가 2500에 근접한 시퀀스배열이 극 소수 있음 (1개) - 242번 인덱스의 시퀀스 배열\n",
    "    - 1500 ~ 2000 은 최소 5개의 시퀀스가 존재\n",
    "    - 제거는 추후 테스트 셋을 고려하지 못한 상태에 이를 수 있음\n",
    "##### 결론 : 2450의 시퀀스 길이가 최대이기에 위의 값을 통해 시퀀스 패딩을 진행한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2450\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sequences_len = []\n",
    "# 시퀀스 길이 배열\n",
    "for i in range(len(train_sequences)):\n",
    "    sequences_len.append(len(train_sequences[i]))\n",
    "    if sequences_len[i] > 2000:\n",
    "        print(sequences_len[i])\n",
    "\n",
    "max_seq_len = 2450\n",
    "\n",
    "padding_train_sequence = pad_sequences(train_sequences, padding='post')\n",
    "padding_test_sequence = pad_sequences(test_sequences, maxlen=2450, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- numpy 배열 형태로 변환\n",
    "- 2차원은 일반적으로 lstm에서 차원을 지정하고 인풋값으로 넣어줘야함\n",
    "    - lstm 은 일반적으로 3차원 입력을 기대함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_data = np.array(train_label_data)\n",
    "test_label_data = np.array(test_label_data)\n",
    "\n",
    "padding_train_sequence = np.array(padding_train_sequence)\n",
    "padding_test_sequence = np.array(padding_test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dropout, BatchNormalization, Dense, Embedding, GlobalAveragePooling1D\n",
    "\n",
    "class base_model(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dims, first_dim, second_dim):\n",
    "        \n",
    "        super(base_model, self).__init__()\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, embedding_dims)\n",
    "\n",
    "        self.pooling_layer = GlobalAveragePooling1D()\n",
    "\n",
    "        self.first_dense = Dense(first_dim, activation='relu')\n",
    "\n",
    "        self.final_dense = Dense(second_dim, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        inputs = tf.cast(inputs, dtype=tf.float32)\n",
    "\n",
    "        x = self.embedding(inputs)\n",
    "\n",
    "        x = self.pooling_layer(x)\n",
    "\n",
    "        x = self.first_dense(x)\n",
    "\n",
    "        return self.final_dense(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"base_model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_15 (Embedding)    multiple                  1432432   \n",
      "                                                                 \n",
      " global_average_pooling1d_8   multiple                 0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_23 (Dense)            multiple                  408       \n",
      "                                                                 \n",
      " dense_24 (Dense)            multiple                  25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,432,865\n",
      "Trainable params: 1,432,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = base_model(vocab_size, 16, 24, 1)\n",
    "base_model.build(input_shape=(None, 2450))\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.6894 - accuracy: 0.5474 - val_loss: 0.6727 - val_accuracy: 0.5778\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5640 - accuracy: 0.7670 - val_loss: 0.4781 - val_accuracy: 0.8080\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.3882 - accuracy: 0.8559 - val_loss: 0.3694 - val_accuracy: 0.8602\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.3059 - accuracy: 0.8855 - val_loss: 0.3376 - val_accuracy: 0.8648\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.2651 - accuracy: 0.8998 - val_loss: 0.3141 - val_accuracy: 0.8740\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.2334 - accuracy: 0.9134 - val_loss: 0.3029 - val_accuracy: 0.8782\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.2096 - accuracy: 0.9228 - val_loss: 0.2928 - val_accuracy: 0.8820\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1886 - accuracy: 0.9324 - val_loss: 0.2939 - val_accuracy: 0.8831\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1715 - accuracy: 0.9380 - val_loss: 0.2873 - val_accuracy: 0.8869\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1550 - accuracy: 0.9444 - val_loss: 0.2904 - val_accuracy: 0.8860\n",
      "Epoch 11/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1405 - accuracy: 0.9518 - val_loss: 0.2918 - val_accuracy: 0.8871\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1277 - accuracy: 0.9562 - val_loss: 0.3285 - val_accuracy: 0.8717\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1170 - accuracy: 0.9614 - val_loss: 0.3109 - val_accuracy: 0.8824\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1042 - accuracy: 0.9658 - val_loss: 0.3206 - val_accuracy: 0.8808\n",
      "Epoch 15/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0945 - accuracy: 0.9689 - val_loss: 0.3184 - val_accuracy: 0.8846\n",
      "Epoch 16/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0889 - accuracy: 0.9718 - val_loss: 0.3323 - val_accuracy: 0.8810\n",
      "Epoch 17/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0795 - accuracy: 0.9750 - val_loss: 0.3710 - val_accuracy: 0.8683\n",
      "Epoch 18/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0727 - accuracy: 0.9784 - val_loss: 0.3585 - val_accuracy: 0.8774\n",
      "Epoch 19/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0641 - accuracy: 0.9814 - val_loss: 0.3612 - val_accuracy: 0.8801\n",
      "Epoch 20/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0600 - accuracy: 0.9830 - val_loss: 0.3812 - val_accuracy: 0.8756\n",
      "Epoch 21/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0534 - accuracy: 0.9851 - val_loss: 0.4099 - val_accuracy: 0.8676\n",
      "Epoch 22/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0477 - accuracy: 0.9871 - val_loss: 0.4090 - val_accuracy: 0.8720\n",
      "Epoch 23/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0449 - accuracy: 0.9884 - val_loss: 0.4260 - val_accuracy: 0.8740\n",
      "Epoch 24/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0400 - accuracy: 0.9892 - val_loss: 0.4422 - val_accuracy: 0.8729\n",
      "Epoch 25/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0361 - accuracy: 0.9910 - val_loss: 0.4550 - val_accuracy: 0.8724\n",
      "Epoch 26/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0343 - accuracy: 0.9913 - val_loss: 0.4599 - val_accuracy: 0.8696\n",
      "Epoch 27/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0308 - accuracy: 0.9925 - val_loss: 0.4751 - val_accuracy: 0.8671\n",
      "Epoch 28/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0277 - accuracy: 0.9935 - val_loss: 0.4882 - val_accuracy: 0.8700\n",
      "Epoch 29/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0255 - accuracy: 0.9940 - val_loss: 0.5118 - val_accuracy: 0.8679\n",
      "Epoch 30/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0243 - accuracy: 0.9941 - val_loss: 0.5207 - val_accuracy: 0.8679\n",
      "Epoch 31/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0223 - accuracy: 0.9946 - val_loss: 0.5473 - val_accuracy: 0.8667\n",
      "Epoch 32/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0197 - accuracy: 0.9954 - val_loss: 0.5746 - val_accuracy: 0.8640\n",
      "Epoch 33/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.5666 - val_accuracy: 0.8622\n",
      "Epoch 34/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0163 - accuracy: 0.9963 - val_loss: 0.5930 - val_accuracy: 0.8643\n",
      "Epoch 35/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0149 - accuracy: 0.9965 - val_loss: 0.5997 - val_accuracy: 0.8596\n",
      "Epoch 36/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0144 - accuracy: 0.9966 - val_loss: 0.6136 - val_accuracy: 0.8597\n",
      "Epoch 37/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0121 - accuracy: 0.9976 - val_loss: 0.6359 - val_accuracy: 0.8619\n",
      "Epoch 38/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0106 - accuracy: 0.9981 - val_loss: 0.6562 - val_accuracy: 0.8529\n",
      "Epoch 39/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0095 - accuracy: 0.9984 - val_loss: 0.6610 - val_accuracy: 0.8567\n",
      "Epoch 40/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0093 - accuracy: 0.9984 - val_loss: 0.6955 - val_accuracy: 0.8590\n",
      "Epoch 41/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0097 - accuracy: 0.9981 - val_loss: 0.7101 - val_accuracy: 0.8578\n",
      "Epoch 42/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0080 - accuracy: 0.9986 - val_loss: 0.7250 - val_accuracy: 0.8509\n",
      "Epoch 43/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0081 - accuracy: 0.9984 - val_loss: 0.7409 - val_accuracy: 0.8500\n",
      "Epoch 44/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0077 - accuracy: 0.9984 - val_loss: 0.7459 - val_accuracy: 0.8553\n",
      "Epoch 45/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0060 - accuracy: 0.9988 - val_loss: 0.7689 - val_accuracy: 0.8546\n",
      "Epoch 46/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0074 - accuracy: 0.9983 - val_loss: 0.8033 - val_accuracy: 0.8452\n",
      "Epoch 47/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.8104 - val_accuracy: 0.8464\n",
      "Epoch 48/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 0.8174 - val_accuracy: 0.8522\n",
      "Epoch 49/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 0.8335 - val_accuracy: 0.8519\n",
      "Epoch 50/50\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0037 - accuracy: 0.9992 - val_loss: 0.8761 - val_accuracy: 0.8514\n"
     ]
    }
   ],
   "source": [
    "embedding_record = base_model.fit(x = padding_train_sequence,\n",
    "                                  y = train_label_data,\n",
    "                                  validation_data=(padding_test_sequence, test_label_data),\n",
    "                                  epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dropout, BatchNormalization, Dense, Embedding, GlobalAveragePooling1D\n",
    "\n",
    "class lstm_model(tf.keras.Model):\n",
    "    def __init__(self, word_idx, embedding_dims, first_unit):\n",
    "        super(lstm_model, self).__init__()\n",
    "\n",
    "        self.embedding_layer = Embedding(word_idx, embedding_dims)\n",
    "\n",
    "        self.first_lstm = LSTM(first_unit, dropout=0.3)\n",
    "\n",
    "        self.dense = Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        inputs = tf.cast(inputs, dtype=tf.float32)\n",
    "\n",
    "        x = self.embedding_layer(inputs)\n",
    "\n",
    "        x = self.first_lstm(x)\n",
    "\n",
    "        return self.dense(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "first_units = 128\n",
    "vocab_size = len(word_index)\n",
    "embedding_dims = 16\n",
    "\n",
    "adam = Adam(learning_rate=0.01)\n",
    "\n",
    "model = lstm_model(word_idx=vocab_size, embedding_dims=embedding_dims, first_unit=first_units)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lstm_model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    multiple                  1432432   \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               multiple                  74240     \n",
      "                                                                 \n",
      " dense_14 (Dense)            multiple                  129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,506,801\n",
      "Trainable params: 1,506,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build(input_shape=(None, 2450))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "782/782 [==============================] - 105s 133ms/step - loss: 0.6933 - accuracy: 0.5015 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - 100s 128ms/step - loss: 0.6932 - accuracy: 0.5021 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 100s 129ms/step - loss: 0.6932 - accuracy: 0.5007 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 103s 132ms/step - loss: 0.6932 - accuracy: 0.4951 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 102s 130ms/step - loss: 0.6932 - accuracy: 0.4989 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 115s 147ms/step - loss: 0.6932 - accuracy: 0.4976 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 133s 170ms/step - loss: 0.6932 - accuracy: 0.4970 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 98s 125ms/step - loss: 0.6932 - accuracy: 0.5012 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 9/50\n",
      "230/782 [=======>......................] - ETA: 49s - loss: 0.6932 - accuracy: 0.5024"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m one_lstm_record \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpadding_train_sequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                            \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_label_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpadding_test_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_label_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Roaming\\Python\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Roaming\\Python\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Roaming\\Python\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Roaming\\Python\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Roaming\\Python\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Roaming\\Python\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Roaming\\Python\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Roaming\\Python\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Roaming\\Python\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "one_lstm_record = model.fit(x = padding_train_sequence,\n",
    "                            y= train_label_data, \n",
    "                            validation_data=(padding_test_sequence, test_label_data), \n",
    "                            epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2450\n"
     ]
    }
   ],
   "source": [
    "def draw_graph(history, name):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    plt.title(name + ' acc evaluate')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('accuracy')\n",
    "    ax1.plot(history.history['accuracy'], color='blue')\n",
    "    ax1.plot(history.history['val_accuracy'],color='red')\n",
    "    ax1.tick_params(axis='y',)\n",
    "    ax1.legend(['acc', 'val_acc'], loc='upper left')\n",
    "\n",
    "    ax2 = ax1.twinx()  # x축을 공유하는 두 번째 축을 생성합니다.\n",
    "\n",
    "    ax2.set_ylabel('loss')\n",
    "    ax2.plot(history.history['loss'], color='blue', linestyle='dashed')\n",
    "    ax2.plot(history.history['val_loss'], color='red', linestyle='dashed')\n",
    "    ax2.tick_params(axis='y')\n",
    "    ax2.legend(['loss', 'val_loss'], loc='upper right')\n",
    "\n",
    "    fig.tight_layout()  # 그래프가 겹치지 않도록 레이아웃을 조정합니다.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(one_lstm_record, 'unity-lstm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
