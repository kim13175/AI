{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, Bidirectional, LSTM, Dropout, BatchNormalization, Masking\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라벨 데이터 작업\n",
    " - No Finding : 특이 점을 찾지 못한 경우\n",
    "    - 반대의 경우는 특이 점을 찾은 경우 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 227827 entries, 0 to 227826\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype\n",
      "---  ------    --------------   -----\n",
      " 0   study_id  227827 non-null  int64\n",
      " 1   Finding   227827 non-null  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 3.5 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_7420\\4232206364.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  label_data_noFinding['No Finding'] = label_data_noFinding['No Finding'].fillna(0)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_7420\\4232206364.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  label_data_noFinding['Finding'] = label_data_noFinding['No Finding'].apply(lambda x: 0 if x == 1 else 1)\n"
     ]
    }
   ],
   "source": [
    "label_data_path = './data/mimic-cxr-2.0.0-chexpert.csv'\n",
    "\n",
    "label_data_csv = pd.read_csv(label_data_path)\n",
    "# 필요 없는 피처 필터링\n",
    "label_data_filter = label_data_csv.drop(columns=['subject_id', 'Support Devices'])\n",
    "\n",
    "label_data_filter = label_data_filter.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "label_data_noFinding = label_data_filter[['study_id', 'No Finding']]\n",
    "label_data_noFinding['No Finding'] = label_data_noFinding['No Finding'].fillna(0)\n",
    "\n",
    "label_data_noFinding['Finding'] = label_data_noFinding['No Finding'].apply(lambda x: 0 if x == 1 else 1)\n",
    "label_data_Finding = label_data_noFinding[['study_id', 'Finding']]\n",
    "\n",
    "label_data_Finding.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# null 값 확인 \n",
    " - 50%가 넘는 데이터가 남게 됨 (impression, findings)\n",
    " - 11,000 여개 정도의 데이터가 남게 되며 위를 학습시켜도 무난할 것이라 예상 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10464\n"
     ]
    }
   ],
   "source": [
    "ehr_csv_data_path = './data/findings_and_impression.csv'\n",
    "\n",
    "ehr_data_csv = pd.read_csv(ehr_csv_data_path)\n",
    "\n",
    "cnt = 0\n",
    "for i in range(len(ehr_data_csv)):\n",
    "    if pd.isnull(ehr_data_csv['Findings'].iloc[i]) or pd.isnull(ehr_data_csv['Impression'].iloc[i]):\n",
    "        cnt += 1\n",
    "\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 널 값이 있는 행을 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>Finding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50414267</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53189527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53911762</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56699142</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57375967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11720</th>\n",
       "      <td>56459556</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11721</th>\n",
       "      <td>57060480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11722</th>\n",
       "      <td>52257272</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11723</th>\n",
       "      <td>52341872</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11724</th>\n",
       "      <td>56958909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11725 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       study_id  Finding\n",
       "0      50414267        0\n",
       "1      53189527        0\n",
       "2      53911762        0\n",
       "3      56699142        0\n",
       "4      57375967        1\n",
       "...         ...      ...\n",
       "11720  56459556        1\n",
       "11721  57060480        0\n",
       "11722  52257272        0\n",
       "11723  52341872        0\n",
       "11724  56958909        0\n",
       "\n",
       "[11725 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ehr_data_csv_filtered = ehr_data_csv.dropna()\n",
    "\n",
    "merge_data_frame = pd.merge(ehr_data_csv_filtered, label_data_Finding, on=\"study_id\")\n",
    "\n",
    "merge_data_frame[['study_id', 'Finding']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11725"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ehr_data_csv_filtered['Findings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[368,\n",
       " 280,\n",
       " 304,\n",
       " 237,\n",
       " 375,\n",
       " 273,\n",
       " 351,\n",
       " 395,\n",
       " 671,\n",
       " 347,\n",
       " 510,\n",
       " 662,\n",
       " 401,\n",
       " 294,\n",
       " 227,\n",
       " 464,\n",
       " 288,\n",
       " 408,\n",
       " 271,\n",
       " 390,\n",
       " 432,\n",
       " 306,\n",
       " 336,\n",
       " 204,\n",
       " 288,\n",
       " 299,\n",
       " 393,\n",
       " 205,\n",
       " 486,\n",
       " 193,\n",
       " 340,\n",
       " 332,\n",
       " 161,\n",
       " 330,\n",
       " 210,\n",
       " 372,\n",
       " 377,\n",
       " 152,\n",
       " 464,\n",
       " 332,\n",
       " 190,\n",
       " 338,\n",
       " 291,\n",
       " 188,\n",
       " 122,\n",
       " 181,\n",
       " 197,\n",
       " 266,\n",
       " 187,\n",
       " 314,\n",
       " 267,\n",
       " 950,\n",
       " 458,\n",
       " 284,\n",
       " 400,\n",
       " 255,\n",
       " 153,\n",
       " 544,\n",
       " 471,\n",
       " 352,\n",
       " 440,\n",
       " 649,\n",
       " 281,\n",
       " 217,\n",
       " 491,\n",
       " 445,\n",
       " 564,\n",
       " 422,\n",
       " 507,\n",
       " 320,\n",
       " 357,\n",
       " 266,\n",
       " 297,\n",
       " 659,\n",
       " 440,\n",
       " 487,\n",
       " 468,\n",
       " 174,\n",
       " 427,\n",
       " 755,\n",
       " 303,\n",
       " 673,\n",
       " 517,\n",
       " 140,\n",
       " 425,\n",
       " 343,\n",
       " 381,\n",
       " 336,\n",
       " 332,\n",
       " 373,\n",
       " 473,\n",
       " 548,\n",
       " 445,\n",
       " 382,\n",
       " 255,\n",
       " 202,\n",
       " 321,\n",
       " 335,\n",
       " 481,\n",
       " 242,\n",
       " 326,\n",
       " 551,\n",
       " 301,\n",
       " 254,\n",
       " 633,\n",
       " 337,\n",
       " 586,\n",
       " 582,\n",
       " 300,\n",
       " 285,\n",
       " 213,\n",
       " 312,\n",
       " 246,\n",
       " 444,\n",
       " 317,\n",
       " 304,\n",
       " 235,\n",
       " 325,\n",
       " 649,\n",
       " 235,\n",
       " 603,\n",
       " 421,\n",
       " 438,\n",
       " 225,\n",
       " 276,\n",
       " 249,\n",
       " 421,\n",
       " 499,\n",
       " 381,\n",
       " 403,\n",
       " 357,\n",
       " 370,\n",
       " 243,\n",
       " 285,\n",
       " 166,\n",
       " 192,\n",
       " 185,\n",
       " 324,\n",
       " 271,\n",
       " 245,\n",
       " 1214,\n",
       " 355,\n",
       " 426,\n",
       " 340,\n",
       " 177,\n",
       " 262,\n",
       " 286,\n",
       " 284,\n",
       " 438,\n",
       " 459,\n",
       " 331,\n",
       " 486,\n",
       " 188,\n",
       " 348,\n",
       " 441,\n",
       " 693,\n",
       " 638,\n",
       " 168,\n",
       " 415,\n",
       " 235,\n",
       " 158,\n",
       " 244,\n",
       " 272,\n",
       " 289,\n",
       " 156,\n",
       " 335,\n",
       " 607,\n",
       " 498,\n",
       " 210,\n",
       " 327,\n",
       " 447,\n",
       " 338,\n",
       " 269,\n",
       " 429,\n",
       " 417,\n",
       " 335,\n",
       " 308,\n",
       " 209,\n",
       " 203,\n",
       " 259,\n",
       " 262,\n",
       " 330,\n",
       " 292,\n",
       " 211,\n",
       " 337,\n",
       " 434,\n",
       " 297,\n",
       " 363,\n",
       " 63,\n",
       " 767,\n",
       " 401,\n",
       " 468,\n",
       " 363,\n",
       " 62,\n",
       " 326,\n",
       " 496,\n",
       " 470,\n",
       " 203,\n",
       " 202,\n",
       " 231,\n",
       " 252,\n",
       " 60,\n",
       " 373,\n",
       " 538,\n",
       " 316,\n",
       " 275,\n",
       " 406,\n",
       " 800,\n",
       " 405,\n",
       " 566,\n",
       " 581,\n",
       " 767,\n",
       " 469,\n",
       " 418,\n",
       " 408,\n",
       " 449,\n",
       " 545,\n",
       " 428,\n",
       " 536,\n",
       " 184,\n",
       " 378,\n",
       " 207,\n",
       " 314,\n",
       " 245,\n",
       " 188,\n",
       " 127,\n",
       " 261,\n",
       " 205,\n",
       " 580,\n",
       " 442,\n",
       " 247,\n",
       " 175,\n",
       " 370,\n",
       " 543,\n",
       " 369,\n",
       " 317,\n",
       " 548,\n",
       " 397,\n",
       " 734,\n",
       " 440,\n",
       " 280,\n",
       " 221,\n",
       " 256,\n",
       " 321,\n",
       " 417,\n",
       " 273,\n",
       " 294,\n",
       " 269,\n",
       " 187,\n",
       " 181,\n",
       " 258,\n",
       " 280,\n",
       " 288,\n",
       " 207,\n",
       " 218,\n",
       " 304,\n",
       " 182,\n",
       " 414,\n",
       " 237,\n",
       " 303,\n",
       " 276,\n",
       " 373,\n",
       " 255,\n",
       " 285,\n",
       " 426,\n",
       " 385,\n",
       " 298,\n",
       " 466,\n",
       " 378,\n",
       " 176,\n",
       " 900,\n",
       " 331,\n",
       " 411,\n",
       " 426,\n",
       " 487,\n",
       " 726,\n",
       " 560,\n",
       " 557,\n",
       " 430,\n",
       " 471,\n",
       " 485,\n",
       " 135,\n",
       " 192,\n",
       " 239,\n",
       " 275,\n",
       " 275,\n",
       " 265,\n",
       " 215,\n",
       " 284,\n",
       " 297,\n",
       " 378,\n",
       " 407,\n",
       " 260,\n",
       " 257,\n",
       " 176,\n",
       " 417,\n",
       " 240,\n",
       " 206,\n",
       " 284,\n",
       " 199,\n",
       " 348,\n",
       " 381,\n",
       " 523,\n",
       " 481,\n",
       " 848,\n",
       " 866,\n",
       " 405,\n",
       " 282,\n",
       " 702,\n",
       " 362,\n",
       " 472,\n",
       " 435,\n",
       " 301,\n",
       " 275,\n",
       " 319,\n",
       " 272,\n",
       " 303,\n",
       " 230,\n",
       " 287,\n",
       " 353,\n",
       " 187,\n",
       " 458,\n",
       " 219,\n",
       " 769,\n",
       " 346,\n",
       " 283,\n",
       " 588,\n",
       " 323,\n",
       " 144,\n",
       " 512,\n",
       " 213,\n",
       " 435,\n",
       " 370,\n",
       " 750,\n",
       " 481,\n",
       " 235,\n",
       " 514,\n",
       " 421,\n",
       " 262,\n",
       " 357,\n",
       " 402,\n",
       " 533,\n",
       " 408,\n",
       " 463,\n",
       " 493,\n",
       " 454,\n",
       " 317,\n",
       " 205,\n",
       " 373,\n",
       " 465,\n",
       " 339,\n",
       " 295,\n",
       " 258,\n",
       " 172,\n",
       " 418,\n",
       " 167,\n",
       " 572,\n",
       " 290,\n",
       " 313,\n",
       " 244,\n",
       " 208,\n",
       " 154,\n",
       " 164,\n",
       " 417,\n",
       " 479,\n",
       " 587,\n",
       " 486,\n",
       " 538,\n",
       " 328,\n",
       " 357,\n",
       " 525,\n",
       " 404,\n",
       " 630,\n",
       " 468,\n",
       " 339,\n",
       " 422,\n",
       " 607,\n",
       " 537,\n",
       " 389,\n",
       " 475,\n",
       " 657,\n",
       " 456,\n",
       " 421,\n",
       " 144,\n",
       " 357,\n",
       " 431,\n",
       " 381,\n",
       " 293,\n",
       " 206,\n",
       " 66,\n",
       " 1015,\n",
       " 240,\n",
       " 313,\n",
       " 565,\n",
       " 608,\n",
       " 505,\n",
       " 418,\n",
       " 451,\n",
       " 165,\n",
       " 233,\n",
       " 316,\n",
       " 264,\n",
       " 188,\n",
       " 405,\n",
       " 209,\n",
       " 633,\n",
       " 712,\n",
       " 285,\n",
       " 167,\n",
       " 61,\n",
       " 425,\n",
       " 228,\n",
       " 192,\n",
       " 471,\n",
       " 177,\n",
       " 159,\n",
       " 233,\n",
       " 186,\n",
       " 285,\n",
       " 286,\n",
       " 181,\n",
       " 379,\n",
       " 410,\n",
       " 362,\n",
       " 214,\n",
       " 94,\n",
       " 154,\n",
       " 184,\n",
       " 129,\n",
       " 242,\n",
       " 205,\n",
       " 137,\n",
       " 155,\n",
       " 460,\n",
       " 190,\n",
       " 242,\n",
       " 303,\n",
       " 185,\n",
       " 306,\n",
       " 265,\n",
       " 52,\n",
       " 438,\n",
       " 63,\n",
       " 332,\n",
       " 278,\n",
       " 141,\n",
       " 188,\n",
       " 315,\n",
       " 284,\n",
       " 417,\n",
       " 299,\n",
       " 282,\n",
       " 310,\n",
       " 478,\n",
       " 371,\n",
       " 545,\n",
       " 464,\n",
       " 527,\n",
       " 349,\n",
       " 198,\n",
       " 208,\n",
       " 262,\n",
       " 337,\n",
       " 437,\n",
       " 262,\n",
       " 391,\n",
       " 357,\n",
       " 310,\n",
       " 167,\n",
       " 239,\n",
       " 281,\n",
       " 55,\n",
       " 1154,\n",
       " 354,\n",
       " 467,\n",
       " 497,\n",
       " 273,\n",
       " 566,\n",
       " 465,\n",
       " 324,\n",
       " 215,\n",
       " 400,\n",
       " 65,\n",
       " 168,\n",
       " 238,\n",
       " 242,\n",
       " 398,\n",
       " 435,\n",
       " 336,\n",
       " 382,\n",
       " 199,\n",
       " 339,\n",
       " 184,\n",
       " 177,\n",
       " 225,\n",
       " 178,\n",
       " 315,\n",
       " 387,\n",
       " 603,\n",
       " 665,\n",
       " 200,\n",
       " 138,\n",
       " 312,\n",
       " 143,\n",
       " 480,\n",
       " 173,\n",
       " 200,\n",
       " 297,\n",
       " 232,\n",
       " 318,\n",
       " 461,\n",
       " 317,\n",
       " 247,\n",
       " 182,\n",
       " 211,\n",
       " 230,\n",
       " 148,\n",
       " 287,\n",
       " 267,\n",
       " 238,\n",
       " 257,\n",
       " 324,\n",
       " 361,\n",
       " 319,\n",
       " 165,\n",
       " 226,\n",
       " 258,\n",
       " 352,\n",
       " 315,\n",
       " 397,\n",
       " 710,\n",
       " 415,\n",
       " 426,\n",
       " 214,\n",
       " 414,\n",
       " 400,\n",
       " 383,\n",
       " 339,\n",
       " 539,\n",
       " 493,\n",
       " 325,\n",
       " 54,\n",
       " 574,\n",
       " 372,\n",
       " 446,\n",
       " 175,\n",
       " 353,\n",
       " 227,\n",
       " 229,\n",
       " 282,\n",
       " 55,\n",
       " 240,\n",
       " 494,\n",
       " 467,\n",
       " 104,\n",
       " 361,\n",
       " 603,\n",
       " 150,\n",
       " 344,\n",
       " 189,\n",
       " 218,\n",
       " 447,\n",
       " 165,\n",
       " 194,\n",
       " 224,\n",
       " 279,\n",
       " 705,\n",
       " 151,\n",
       " 365,\n",
       " 367,\n",
       " 437,\n",
       " 139,\n",
       " 146,\n",
       " 437,\n",
       " 277,\n",
       " 325,\n",
       " 187,\n",
       " 150,\n",
       " 220,\n",
       " 185,\n",
       " 527,\n",
       " 475,\n",
       " 240,\n",
       " 268,\n",
       " 437,\n",
       " 618,\n",
       " 169,\n",
       " 185,\n",
       " 400,\n",
       " 415,\n",
       " 243,\n",
       " 390,\n",
       " 192,\n",
       " 333,\n",
       " 213,\n",
       " 196,\n",
       " 650,\n",
       " 225,\n",
       " 223,\n",
       " 126,\n",
       " 192,\n",
       " 408,\n",
       " 311,\n",
       " 246,\n",
       " 302,\n",
       " 246,\n",
       " 213,\n",
       " 195,\n",
       " 305,\n",
       " 220,\n",
       " 247,\n",
       " 61,\n",
       " 250,\n",
       " 59,\n",
       " 366,\n",
       " 667,\n",
       " 491,\n",
       " 716,\n",
       " 556,\n",
       " 345,\n",
       " 232,\n",
       " 236,\n",
       " 249,\n",
       " 197,\n",
       " 360,\n",
       " 232,\n",
       " 459,\n",
       " 348,\n",
       " 572,\n",
       " 370,\n",
       " 730,\n",
       " 223,\n",
       " 328,\n",
       " 376,\n",
       " 534,\n",
       " 376,\n",
       " 532,\n",
       " 161,\n",
       " 380,\n",
       " 466,\n",
       " 298,\n",
       " 385,\n",
       " 425,\n",
       " 531,\n",
       " 896,\n",
       " 333,\n",
       " 535,\n",
       " 312,\n",
       " 511,\n",
       " 376,\n",
       " 421,\n",
       " 433,\n",
       " 312,\n",
       " 130,\n",
       " 259,\n",
       " 270,\n",
       " 483,\n",
       " 428,\n",
       " 527,\n",
       " 406,\n",
       " 435,\n",
       " 535,\n",
       " 207,\n",
       " 301,\n",
       " 276,\n",
       " 196,\n",
       " 288,\n",
       " 217,\n",
       " 255,\n",
       " 272,\n",
       " 439,\n",
       " 168,\n",
       " 172,\n",
       " 162,\n",
       " 220,\n",
       " 354,\n",
       " 323,\n",
       " 402,\n",
       " 391,\n",
       " 209,\n",
       " 334,\n",
       " 206,\n",
       " 291,\n",
       " 124,\n",
       " 265,\n",
       " 241,\n",
       " 314,\n",
       " 308,\n",
       " 202,\n",
       " 321,\n",
       " 126,\n",
       " 171,\n",
       " 121,\n",
       " 490,\n",
       " 315,\n",
       " 305,\n",
       " 233,\n",
       " 321,\n",
       " 221,\n",
       " 314,\n",
       " 304,\n",
       " 399,\n",
       " 269,\n",
       " 484,\n",
       " 326,\n",
       " 176,\n",
       " 142,\n",
       " 302,\n",
       " 285,\n",
       " 158,\n",
       " 68,\n",
       " 308,\n",
       " 215,\n",
       " 189,\n",
       " 145,\n",
       " 212,\n",
       " 381,\n",
       " 362,\n",
       " 403,\n",
       " 525,\n",
       " 355,\n",
       " 319,\n",
       " 366,\n",
       " 300,\n",
       " 341,\n",
       " 419,\n",
       " 260,\n",
       " 446,\n",
       " 285,\n",
       " 226,\n",
       " 252,\n",
       " 414,\n",
       " 302,\n",
       " 569,\n",
       " 191,\n",
       " 444,\n",
       " 255,\n",
       " 289,\n",
       " 214,\n",
       " 225,\n",
       " 215,\n",
       " 259,\n",
       " 57,\n",
       " 388,\n",
       " 432,\n",
       " 358,\n",
       " 239,\n",
       " 272,\n",
       " 225,\n",
       " 269,\n",
       " 248,\n",
       " 292,\n",
       " 618,\n",
       " 401,\n",
       " 183,\n",
       " 618,\n",
       " 402,\n",
       " 108,\n",
       " 235,\n",
       " 133,\n",
       " 127,\n",
       " 403,\n",
       " 208,\n",
       " 242,\n",
       " 238,\n",
       " 430,\n",
       " 322,\n",
       " 138,\n",
       " 369,\n",
       " 339,\n",
       " 322,\n",
       " 318,\n",
       " 233,\n",
       " 213,\n",
       " 213,\n",
       " 135,\n",
       " 160,\n",
       " 566,\n",
       " 165,\n",
       " 197,\n",
       " 268,\n",
       " 440,\n",
       " 374,\n",
       " 479,\n",
       " 747,\n",
       " 732,\n",
       " 240,\n",
       " 287,\n",
       " 417,\n",
       " 227,\n",
       " 208,\n",
       " 236,\n",
       " 385,\n",
       " 324,\n",
       " 209,\n",
       " 317,\n",
       " 493,\n",
       " 683,\n",
       " 328,\n",
       " 223,\n",
       " 419,\n",
       " 297,\n",
       " 433,\n",
       " 229,\n",
       " 275,\n",
       " 250,\n",
       " 413,\n",
       " 485,\n",
       " 319,\n",
       " 639,\n",
       " 476,\n",
       " 248,\n",
       " 334,\n",
       " 448,\n",
       " 202,\n",
       " 298,\n",
       " 498,\n",
       " 549,\n",
       " 331,\n",
       " 269,\n",
       " 362,\n",
       " 430,\n",
       " 339,\n",
       " 291,\n",
       " 528,\n",
       " 336,\n",
       " 399,\n",
       " 309,\n",
       " 296,\n",
       " 255,\n",
       " 320,\n",
       " 167,\n",
       " 372,\n",
       " 340,\n",
       " 313,\n",
       " 513,\n",
       " 461,\n",
       " 465,\n",
       " 107,\n",
       " 158,\n",
       " 306,\n",
       " 310,\n",
       " 288,\n",
       " 211,\n",
       " 232,\n",
       " 409,\n",
       " 289,\n",
       " 304,\n",
       " 396,\n",
       " 446,\n",
       " 520,\n",
       " 445,\n",
       " 498,\n",
       " 259,\n",
       " 365,\n",
       " 312,\n",
       " 213,\n",
       " 899,\n",
       " 365,\n",
       " 239,\n",
       " 187,\n",
       " 375,\n",
       " 468,\n",
       " 402,\n",
       " 242,\n",
       " 196,\n",
       " 213,\n",
       " 324,\n",
       " 317,\n",
       " 478,\n",
       " 277,\n",
       " 162,\n",
       " 135,\n",
       " 131,\n",
       " 189,\n",
       " 193,\n",
       " 352,\n",
       " 340,\n",
       " 261,\n",
       " 921,\n",
       " 408,\n",
       " 1068,\n",
       " 337,\n",
       " 295,\n",
       " 376,\n",
       " 361,\n",
       " 294,\n",
       " 3,\n",
       " 198,\n",
       " 231,\n",
       " 323,\n",
       " 381,\n",
       " 381,\n",
       " 335,\n",
       " 438,\n",
       " 144,\n",
       " 593,\n",
       " 256,\n",
       " 487,\n",
       " 321,\n",
       " 499,\n",
       " 312,\n",
       " 360,\n",
       " 178,\n",
       " 263,\n",
       " 240,\n",
       " 289,\n",
       " 190,\n",
       " 795,\n",
       " 314,\n",
       " 311,\n",
       " 208,\n",
       " 252,\n",
       " 249,\n",
       " 327,\n",
       " 227,\n",
       " 313,\n",
       " 305,\n",
       " 294,\n",
       " 154,\n",
       " 288,\n",
       " 154,\n",
       " 253,\n",
       " 161,\n",
       " 568,\n",
       " 371,\n",
       " 432,\n",
       " 224,\n",
       " 147,\n",
       " 206,\n",
       " 252,\n",
       " 187,\n",
       " 86,\n",
       " 155,\n",
       " 772,\n",
       " 388,\n",
       " 347,\n",
       " 357,\n",
       " 181,\n",
       " 249,\n",
       " 357,\n",
       " 401,\n",
       " 324,\n",
       " 530,\n",
       " 702,\n",
       " 258,\n",
       " 601,\n",
       " 328,\n",
       " 336,\n",
       " 457,\n",
       " 404,\n",
       " 486,\n",
       " 269,\n",
       " 462,\n",
       " 457,\n",
       " 418,\n",
       " 527,\n",
       " 212,\n",
       " 240,\n",
       " 150,\n",
       " 508,\n",
       " 343,\n",
       " 358,\n",
       " 525,\n",
       " 653,\n",
       " 629,\n",
       " 458,\n",
       " 482,\n",
       " 271,\n",
       " 275,\n",
       " 299,\n",
       " 363,\n",
       " 151,\n",
       " 409,\n",
       " 248,\n",
       " 423,\n",
       " 230,\n",
       " 308,\n",
       " 180,\n",
       " 349,\n",
       " 329,\n",
       " 195,\n",
       " 248,\n",
       " 193,\n",
       " 220,\n",
       " 467,\n",
       " 404,\n",
       " 372,\n",
       " 773,\n",
       " 249,\n",
       " 68,\n",
       " 301,\n",
       " 148,\n",
       " 213,\n",
       " 145,\n",
       " 240,\n",
       " 331,\n",
       " 333,\n",
       " 1231,\n",
       " 401,\n",
       " ...]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_length = []\n",
    "\n",
    "for finding in ehr_data_csv_filtered['Findings']:\n",
    "    finding_length = len(finding)\n",
    "    sentence_length.append(finding_length)\n",
    "\n",
    "sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no focal consolidation, pleural effusion or pneumothorax.  Bilateral\n",
      " nodular opacities that most likely represent nipple shadows. The\n",
      " cardiomediastinal silhouette is normal.  Clips project over the left lung,\n",
      " potentially within the breast. The imaged upper abdomen is unremarkable.\n",
      " Chronic deformity of the posterior left sixth and seventh ribs are noted.\n",
      "\n",
      "No acute cardiopulmonary process.\n"
     ]
    }
   ],
   "source": [
    "findings_lst = []\n",
    "ehr_data_csv_filtered['Findings'].astype('str')\n",
    "for string in ehr_data_csv_filtered['Findings']:\n",
    "    findings_lst.append(string)\n",
    "\n",
    "impression_lst = []\n",
    "ehr_data_csv_filtered['Impression'].astype('str')\n",
    "for string in ehr_data_csv_filtered['Impression']:\n",
    "    impression_lst.append(string)\n",
    "\n",
    "print(findings_lst[0], end='\\n')\n",
    "print()\n",
    "print(impression_lst[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리\n",
    " - 1. ___ -> name 변수로 치환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no focal consolidation, pleural effusion or pneumothorax.  Bilateral\n",
      " nodular opacities that most likely represent nipple shadows. The\n",
      " cardiomediastinal silhouette is normal.  Clips project over the left lung,\n",
      " potentially within the breast. The imaged upper abdomen is unremarkable.\n",
      " Chronic deformity of the posterior left sixth and seventh ribs are noted.\n",
      "No acute cardiopulmonary process.\n"
     ]
    }
   ],
   "source": [
    "# ___ -> name 으로 변환\n",
    "def replace_data(texts):\n",
    "    special_char = \"___\"\n",
    "    return texts.replace(special_char, \"name\")\n",
    "\n",
    "\n",
    "replace_finding_lst = []\n",
    "for i in range(len(findings_lst)):\n",
    "    replace_finding_lst.append(replace_data(findings_lst[i]))\n",
    "\n",
    "replace_impression_lst = []\n",
    "for i in range(len(impression_lst)):\n",
    "    replace_impression_lst.append(replace_data(impression_lst[i]))\n",
    "\n",
    "print(replace_finding_lst[0], end='\\n')\n",
    "print(replace_impression_lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(replace_finding_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정규식 표현을 통한 특수문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no focal consolidation pleural effusion or pneumothorax bilateral nodular opacities that most likely represent nipple shadows the cardiomediastinal silhouette is normal clips project over the left lung potentially within the breast the imaged upper abdomen is unremarkable chronic deformity of the posterior left sixth and seventh ribs are noted \n"
     ]
    }
   ],
   "source": [
    "# 모든 텍스트 소문자변환, 특수문자 정규표현식을 제거\n",
    "def re_text_preprocessing(lst):\n",
    "     re_cleaned_list = []\n",
    "     for text in lst:\n",
    "          text = text.lower()\n",
    "          words = text.split()\n",
    "          refine_text = \"\"\n",
    "          for word in words:\n",
    "               refine_text += word + ' '\n",
    "          re_cleaned_text = re.sub(r'[^\\w\\s]', '', refine_text)\n",
    "          re_cleaned_list.append(re_cleaned_text)\n",
    "     return re_cleaned_list\n",
    "\n",
    "replace_finding_lst = re_text_preprocessing(replace_finding_lst)\n",
    "\n",
    "print(replace_finding_lst[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어 제거\n",
    "    - 불용어가 제거된 것을 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 :  there is no focal consolidation pleural effusion or pneumothorax bilateral nodular opacities that most likely represent nipple shadows the cardiomediastinal silhouette is normal clips project over the left lung potentially within the breast the imaged upper abdomen is unremarkable chronic deformity of the posterior left sixth and seventh ribs are noted \n",
      "불용어 제거 후 :  focal consolidation pleural effusion pneumothorax bilateral nodular opacities likely represent nipple shadows cardiomediastinal silhouette normal clips project left lung potentially within breast imaged upper abdomen unremarkable chronic deformity posterior left sixth seventh ribs noted \n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(texts):\n",
    "    rm_sw_text = \"\"\n",
    "    words = texts.split()\n",
    "    filtered_text = \"\"\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            filtered_text += word + ' '\n",
    "    rm_sw_text += filtered_text\n",
    "    return rm_sw_text \n",
    "\n",
    "replace_finding_lst_v2 = []\n",
    "for idx in range(len(replace_finding_lst)):\n",
    "    replace_finding_lst_v2.append(remove_stopwords(replace_finding_lst[idx]))\n",
    "# 완성된 문장 집합\n",
    "print(\"불용어 제거 전 : \", replace_finding_lst[0])\n",
    "print(\"불용어 제거 후 : \", replace_finding_lst_v2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사전 학습된 토크나이저 사용\n",
    " - 참고 레퍼런스 : https://wikidocs.net/166801\n",
    " - 트레인 셋과 테스트 셋 분할\n",
    " - 사전 학습 토큰화의 단어 어휘 개수 : 28996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Roaming\\Python\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# 사용 데이터 토크나이징\\nencoded_inputs = tokenizer(replace_finding_lst_v2, return_tensors='np', padding=True, truncation=True)\\ninput_ids = encoded_inputs['input_ids']\\nattention_mask = encoded_inputs['attention_mask']\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 의료계에서 사용하는 토큰화 모델\n",
    "tokenizer_model_name = 'dmis-lab/biobert-base-cased-v1.2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "print(vocab_size)\n",
    "'''\n",
    "# 사용 데이터 토크나이징\n",
    "encoded_inputs = tokenizer(replace_finding_lst_v2, return_tensors='np', padding=True, truncation=True)\n",
    "input_ids = encoded_inputs['input_ids']\n",
    "attention_mask = encoded_inputs['attention_mask']\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화된 문장들 텐서로 변환\n",
    " - 문장들 텐서로 변환 시킨 것을 input\n",
    " - Finding 피처를 label\n",
    "### 시퀀스분석\n",
    " - 문장 시퀀스화 할 경우 최대 길이 = 1472\n",
    "  - maxlen = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "1472\n"
     ]
    }
   ],
   "source": [
    "tokens_list = []\n",
    "for text in replace_finding_lst_v2:\n",
    "    tokens = tokenizer.encode(text, truncation=True, padding='max_length', max_length=512)\n",
    "    tokens_list.append(tokens)\n",
    "\n",
    "tokens_len = []\n",
    "for text_len in replace_finding_lst_v2:\n",
    "    tokens_len.append(len(text_len))\n",
    "\n",
    "sns.histplot(data=tokens_len)\n",
    "plt.xlabel('sequences_len')\n",
    "plt.ylabel('sequences_count')\n",
    "plt.show()\n",
    "\n",
    "cnt = 0\n",
    "for data in tokens_len:\n",
    "    if data > 1000:\n",
    "        cnt += 1\n",
    "\n",
    "print(cnt)\n",
    "print(max(tokens_len))\n",
    "\n",
    "max_len = 1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 입력값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11725, 1500) (11725,)\n"
     ]
    }
   ],
   "source": [
    "tokens_padded = pad_sequences(tokens_list, maxlen=max_len)\n",
    "\n",
    "input_value = tokens_padded\n",
    "\n",
    "# target\n",
    "label = merge_data_frame['Finding']\n",
    "\n",
    "print(input_value.shape, label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 트레인, 검증 셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9380, 1500) (2345, 1500) (9380,) (2345,)\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "\n",
    "input_train, input_test, label_train, label_test = train_test_split(input_value, label, test_size=0.2, random_state=seed)  \n",
    "\n",
    "print(input_train.shape, input_test.shape, label_train.shape, label_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 모델 클래스 정의\n",
    " - 참고 레퍼런스 https://keras.io/api/layers/recurrent_layers/lstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class medical_LSTM(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_units, lstm_units, dense_input_dim, dense_output_dim, dropout_rate, l2_reg_rate):\n",
    "        super(medical_LSTM, self).__init__()\n",
    "\n",
    "        self.embedding_layer = Embedding(vocab_size, embedding_units)\n",
    "\n",
    "        self.first_lstm = Bidirectional(LSTM(lstm_units, dropout=dropout_rate, return_sequences=True))\n",
    "\n",
    "        self.last_lstm = Bidirectional(LSTM(lstm_units, dropout=dropout_rate, return_sequences=False))\n",
    "\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        \n",
    "        self.batch_normalization = BatchNormalization()\n",
    "\n",
    "        self.first_dense = Dense(dense_input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_reg_rate))\n",
    "\n",
    "        self.last_dense = Dense(dense_output_dim, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(l2_reg_rate))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "\n",
    "        inputs = tf.cast(inputs, dtype=tf.float32)\n",
    "\n",
    "        embedding_output = self.embedding_layer(inputs)\n",
    "\n",
    "        first_lstm_output = self.first_lstm(embedding_output)\n",
    "\n",
    "        last_lstm_output = self.last_lstm(first_lstm_output)\n",
    "\n",
    "        dropout_output = self.dropout(last_lstm_output)\n",
    "\n",
    "        normalized_output = self.batch_normalization(dropout_output)\n",
    "\n",
    "        first_dense_outputs = self.first_dense(normalized_output)\n",
    "\n",
    "        last_dense_outputs = self.last_dense(first_dense_outputs)\n",
    "\n",
    "        return last_dense_outputs       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from hyperband\\medical_LSTM\\tuner0.json\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'embedding_units does not exist.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch(input_train, label_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(input_test, label_test))\n\u001b[0;32m     30\u001b[0m best_hp \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(num_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding 유닛 수 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_hp\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding_units\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlstm 유닛 수 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_hp\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm_units\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m완전 밀집층 차원 수 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_hp\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdense_input_dim\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Roaming\\Python\\lib\\site-packages\\keras_tuner\\src\\engine\\hyperparameters\\hyperparameters.py:246\u001b[0m, in \u001b[0;36mHyperParameters.get\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is currently inactive.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'embedding_units does not exist.'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from kerastuner.tuners import Hyperband\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "adam = Adam(learning_rate=0.001)\n",
    "\n",
    "def build_model(hp):\n",
    "    model = medical_LSTM(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_units=hp.Int('embedding_units', min_value=32, max_value=128, step=32),\n",
    "        lstm_units=hp.Int('lstm_units', min_value=32, max_value=256, step=32),\n",
    "        dense_input_dim=hp.Int('dense_input_dim', min_value=32, max_value=256, step=32),\n",
    "        dense_output_dim=1,\n",
    "        dropout_rate=hp.Float('dropout_rate', min_value=0.3, max_value=0.5, step=0.01),\n",
    "        l2_reg_rate=hp.Float('l2_reg_rate', min_value=0.001, max_value=0.01, step=0.001)\n",
    "    )\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuner = Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=3,\n",
    "    directory='hyperband',\n",
    "    project_name='medical_LSTM'\n",
    ")\n",
    "\n",
    "tuner.search(input_train, label_train, epochs=3, validation_data=(input_test, label_test))\n",
    "best_hp = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "print(f\"embedding 유닛 수 : {best_hp.get('embedding_units')}\")\n",
    "print(f\"lstm 유닛 수 : {best_hp.get('lstm_units')}\")\n",
    "print(f\"완전 밀집층 차원 수 : {best_hp.get('dense_input_dim')}\")\n",
    "print(f\"dropout_rate : {best_hp.get('dropout_rate')}\")\n",
    "print(f\"l2_reg_rate : {best_hp.get('l2_reg_rate')}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lstm 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sub_lstm(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_units, lstm_units):\n",
    "        super(sub_lstm, self).__init__()\n",
    "\n",
    "        self.embedding_layer = Embedding(vocab_size, embedding_units)\n",
    "\n",
    "        self.lstm_layer = LSTM(lstm_units)\n",
    "\n",
    "        self.output_layer = Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        inputs = tf.cast(inputs, dtype=tf.float32)\n",
    "\n",
    "        x = self.embedding_layer(inputs)\n",
    "\n",
    "        x = self.lstm_layer(x)\n",
    "\n",
    "        outputs = self.output_layer(x)\n",
    "\n",
    "        return outputs            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 Complete [00h 00m 51s]\n",
      "val_accuracy: 0.5206822752952576\n",
      "\n",
      "Best val_accuracy So Far: 0.5206822752952576\n",
      "Total elapsed time: 00h 02m 52s\n",
      "임베딩 유닛 개수 : 96\n",
      "lstm 유닛 개수 : 32\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners import Hyperband\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "adam = Adam(learning_rate=0.01)\n",
    "\n",
    "def build_md(hyperband):\n",
    "    model = sub_lstm(\n",
    "        vocab_size = vocab_size,\n",
    "        embedding_units = hyperband.Int('embedding_units', min_value=32, max_value=128, step=32),\n",
    "        lstm_units = hyperband.Int('lstm_units', min_value=32, max_value=128, step=32),\n",
    "    )\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "searching = Hyperband(\n",
    "    hypermodel = build_md,\n",
    "    objective = 'val_accuracy',\n",
    "    max_epochs = 3,\n",
    "    seed = seed,\n",
    "    directory = 'hyperband',\n",
    "    project_name = 'sub_lstm'\n",
    ")\n",
    "\n",
    "searching.search(input_train, label_train, epochs=3, validation_data=(input_test, label_test))\n",
    "best_params = searching.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"임베딩 유닛 개수 : {best_params.get('embedding_units')}\")\n",
    "print(f\"lstm 유닛 개수 : {best_params.get('lstm_units')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 빌드 및 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sub_lstm_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     multiple                  2783616   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               multiple                  16512     \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,800,161\n",
      "Trainable params: 2,800,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_units = 96\n",
    "lstm_units = 32\n",
    "\n",
    "sub_model = sub_lstm(\n",
    "    vocab_size= vocab_size,\n",
    "    embedding_units= embedding_units,\n",
    "    lstm_units= lstm_units\n",
    ")\n",
    "\n",
    "sub_model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "sub_model.build(input_shape=input_train.shape)\n",
    "sub_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 훈련 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "294/294 [==============================] - 14s 47ms/step - loss: 0.6928 - accuracy: 0.5152 - val_loss: 0.6924 - val_accuracy: 0.5207\n",
      "Epoch 2/20\n",
      "294/294 [==============================] - 14s 48ms/step - loss: 0.6928 - accuracy: 0.5152 - val_loss: 0.6924 - val_accuracy: 0.5207\n",
      "Epoch 3/20\n",
      "294/294 [==============================] - 13s 46ms/step - loss: 0.6928 - accuracy: 0.5152 - val_loss: 0.6923 - val_accuracy: 0.5207\n",
      "Epoch 4/20\n",
      "294/294 [==============================] - 14s 46ms/step - loss: 0.6929 - accuracy: 0.5152 - val_loss: 0.6924 - val_accuracy: 0.5207\n",
      "Epoch 5/20\n",
      "294/294 [==============================] - 13s 46ms/step - loss: 0.6928 - accuracy: 0.5152 - val_loss: 0.6925 - val_accuracy: 0.5207\n",
      "Epoch 6/20\n",
      "294/294 [==============================] - 13s 46ms/step - loss: 0.6927 - accuracy: 0.5152 - val_loss: 0.6925 - val_accuracy: 0.5207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24c443d90c0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "sub_model.fit(x = input_train, \n",
    "              y = label_train,\n",
    "              validation_data=(input_test, label_test),\n",
    "              epochs=20,\n",
    "              callbacks=earlystop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 컴파일\n",
    " - 최적화 도구 adam learning_rate = 0.01\n",
    " - 콜백함수 레퍼런스 : https://deep-deep-deep.tistory.com/1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_units = 256\n",
    "word_size = tokenizer.vocab_size\n",
    "dense_input_dim = 96\n",
    "final_output_dim = 1\n",
    "dropout_rate = 0.0\n",
    "\n",
    "model = medical_LSTM(\n",
    "            lstm_units = lstm_units, \n",
    "            dense_input_dim = dense_input_dim, \n",
    "            dense_output_dim = final_output_dim,\n",
    "            dropout_rate= dropout_rate,\n",
    "            l2_reg_rate=0.0041)\n",
    "\n",
    "model.compile(optimizer = adam, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"medical_lstm_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_9 (Bidirectio  multiple                 528384    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_10 (Bidirecti  multiple                 1574912   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  multiple                 2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_12 (Dense)            multiple                  49248     \n",
      "                                                                 \n",
      " dense_13 (Dense)            multiple                  97        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,154,689\n",
      "Trainable params: 2,153,665\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build(input_shape=(input_train.shape))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "235/235 [==============================] - 27s 107ms/step - loss: 1.0871 - accuracy: 0.6069 - val_loss: 0.9885 - val_accuracy: 0.4835\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 24s 101ms/step - loss: 0.8364 - accuracy: 0.6627 - val_loss: 1.2002 - val_accuracy: 0.4835\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 24s 102ms/step - loss: 0.7377 - accuracy: 0.6635 - val_loss: 0.8148 - val_accuracy: 0.4877\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 24s 101ms/step - loss: 0.6790 - accuracy: 0.6763 - val_loss: 0.8406 - val_accuracy: 0.4851\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 24s 102ms/step - loss: 0.6432 - accuracy: 0.6890 - val_loss: 0.9879 - val_accuracy: 0.4851\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 24s 102ms/step - loss: 0.6204 - accuracy: 0.6924 - val_loss: 0.7794 - val_accuracy: 0.4851\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 24s 102ms/step - loss: 0.6134 - accuracy: 0.6891 - val_loss: 0.7552 - val_accuracy: 0.4979\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 24s 102ms/step - loss: 0.6004 - accuracy: 0.7015 - val_loss: 0.7196 - val_accuracy: 0.4877\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 24s 101ms/step - loss: 0.6037 - accuracy: 0.6960 - val_loss: 0.7552 - val_accuracy: 0.4915\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 24s 104ms/step - loss: 0.5929 - accuracy: 0.6940 - val_loss: 0.8634 - val_accuracy: 0.4936\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 24s 102ms/step - loss: 0.5958 - accuracy: 0.6996 - val_loss: 0.7771 - val_accuracy: 0.4936\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 24s 102ms/step - loss: 0.5893 - accuracy: 0.6988 - val_loss: 0.7548 - val_accuracy: 0.4936\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 24s 102ms/step - loss: 0.5903 - accuracy: 0.7012 - val_loss: 0.8735 - val_accuracy: 0.4877\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 24s 101ms/step - loss: 0.5928 - accuracy: 0.6940 - val_loss: 0.6015 - val_accuracy: 0.6860\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 24s 102ms/step - loss: 0.5900 - accuracy: 0.7006 - val_loss: 0.9858 - val_accuracy: 0.4909\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 24s 101ms/step - loss: 0.5858 - accuracy: 0.7055 - val_loss: 0.7550 - val_accuracy: 0.4941\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 24s 101ms/step - loss: 0.5852 - accuracy: 0.7047 - val_loss: 0.8522 - val_accuracy: 0.4925\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 23s 100ms/step - loss: 0.5891 - accuracy: 0.7022 - val_loss: 1.0095 - val_accuracy: 0.4936\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 24s 100ms/step - loss: 0.5861 - accuracy: 0.7043 - val_loss: 0.7214 - val_accuracy: 0.4931\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 24s 102ms/step - loss: 0.5841 - accuracy: 0.7048 - val_loss: 0.6557 - val_accuracy: 0.6066\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 26s 109ms/step - loss: 0.5866 - accuracy: 0.7030 - val_loss: 1.2990 - val_accuracy: 0.4904\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 25s 105ms/step - loss: 0.5858 - accuracy: 0.7042 - val_loss: 1.0002 - val_accuracy: 0.4936\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 24s 104ms/step - loss: 0.5828 - accuracy: 0.7100 - val_loss: 0.6795 - val_accuracy: 0.4936\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 24s 103ms/step - loss: 0.5849 - accuracy: 0.7058 - val_loss: 0.9420 - val_accuracy: 0.4925\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 25s 105ms/step - loss: 0.5848 - accuracy: 0.7062 - val_loss: 0.7254 - val_accuracy: 0.4941\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 24s 103ms/step - loss: 0.5801 - accuracy: 0.7070 - val_loss: 0.7529 - val_accuracy: 0.4936\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 24s 102ms/step - loss: 0.5819 - accuracy: 0.7055 - val_loss: 0.7417 - val_accuracy: 0.4936\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 24s 102ms/step - loss: 0.5783 - accuracy: 0.7060 - val_loss: 1.2602 - val_accuracy: 0.4936\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 24s 101ms/step - loss: 0.5847 - accuracy: 0.6980 - val_loss: 0.6480 - val_accuracy: 0.6461\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 24s 102ms/step - loss: 0.5860 - accuracy: 0.7020 - val_loss: 0.7100 - val_accuracy: 0.4867\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 24s 102ms/step - loss: 0.5977 - accuracy: 0.6870 - val_loss: 0.7173 - val_accuracy: 0.5165\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 23s 100ms/step - loss: 0.5918 - accuracy: 0.6966 - val_loss: 0.7023 - val_accuracy: 0.5165\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 24s 100ms/step - loss: 0.5868 - accuracy: 0.7072 - val_loss: 0.7839 - val_accuracy: 0.4920\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 24s 100ms/step - loss: 0.5879 - accuracy: 0.7050 - val_loss: 0.7439 - val_accuracy: 0.4781\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 24s 102ms/step - loss: 0.5847 - accuracy: 0.7003 - val_loss: 0.8838 - val_accuracy: 0.4835\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 24s 101ms/step - loss: 0.5844 - accuracy: 0.7068 - val_loss: 0.8730 - val_accuracy: 0.4835\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 24s 101ms/step - loss: 0.5834 - accuracy: 0.7047 - val_loss: 0.8230 - val_accuracy: 0.4883\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 24s 104ms/step - loss: 0.5837 - accuracy: 0.7003 - val_loss: 0.7833 - val_accuracy: 0.4925\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 25s 105ms/step - loss: 0.5836 - accuracy: 0.7055 - val_loss: 0.7076 - val_accuracy: 0.4931\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 24s 101ms/step - loss: 0.5822 - accuracy: 0.7036 - val_loss: 0.6813 - val_accuracy: 0.4941\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 24s 103ms/step - loss: 0.5841 - accuracy: 0.7036 - val_loss: 0.7413 - val_accuracy: 0.4925\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 24s 102ms/step - loss: 0.5838 - accuracy: 0.7040 - val_loss: 0.8397 - val_accuracy: 0.4867\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 24s 102ms/step - loss: 0.5821 - accuracy: 0.7074 - val_loss: 0.7721 - val_accuracy: 0.4920\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 24s 102ms/step - loss: 0.5798 - accuracy: 0.7060 - val_loss: 0.7411 - val_accuracy: 0.4931\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 24s 102ms/step - loss: 0.5802 - accuracy: 0.7062 - val_loss: 0.7007 - val_accuracy: 0.4899\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 24s 102ms/step - loss: 0.5779 - accuracy: 0.7070 - val_loss: 0.6812 - val_accuracy: 0.5666\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 24s 101ms/step - loss: 0.5834 - accuracy: 0.7042 - val_loss: 0.7995 - val_accuracy: 0.4904\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 24s 101ms/step - loss: 0.5797 - accuracy: 0.7082 - val_loss: 0.6445 - val_accuracy: 0.6178\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 24s 101ms/step - loss: 0.5808 - accuracy: 0.7036 - val_loss: 0.7938 - val_accuracy: 0.4915\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 24s 101ms/step - loss: 0.5781 - accuracy: 0.7072 - val_loss: 0.6295 - val_accuracy: 0.6823\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',patience=4)\n",
    "\n",
    "record = model.fit(\n",
    "    input_train,\n",
    "    label_train,\n",
    "    epochs = 50,\n",
    "    batch_size = batch_size,\n",
    "    validation_split = 0.2,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
